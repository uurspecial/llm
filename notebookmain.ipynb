{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"LLM Classification Finetuning åˆ†é¡å¾®èª¿\n-\n**ä»»å‹™é¡å‹ï¼šåˆ†é¡ä»»å‹™ï¼ˆA / B / Tieï¼‰**  \n**ç›®æ¨™ï¼šè¨“ç·´ä¸€å€‹æ–°çš„æ¨¡å‹ï¼Œå»æ¨¡ä»¿äººé¡æ€éº¼åˆ¤æ–·åå¥½ã€‚**\n\n* æ¨¡å‹è¼¸å…¥ï¼šPrompt + Response A + Response B\n* æ¨¡å‹è¼¸å‡ºï¼šåˆ¤æ–· A / B / tieï¼Œèª°çš„å›æ‡‰è¼ƒå¥½\n","metadata":{}},{"cell_type":"markdown","source":"unsloth+llama-3-8bçš„éŒ¯èª¤\n-","metadata":{}},{"cell_type":"code","source":"!pip uninstall -y torch torchvision torchaudio\n!pip install torch==2.6.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n!pip install -U unsloth unsloth_zoo bitsandbytes\n!pip install --upgrade \"transformers>=4.53.0\"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# â—ä¸è¦åœ¨é€™æ®µå‰é¢ import transformers æˆ–å…¶ä»– model\nfrom unsloth import FastLanguageModel\n\nFastLanguageModel.PATCHING_MODE = \"autograd\"  # ğŸ”¥ ä¸€å®šè¦åœ¨ import ä»»ä½• transformers å‰è¨­å®š\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n    max_seq_length = 2048,\n    dtype = None,\n    load_in_4bit = True,\n)\n\nFastLanguageModel.for_training(model)\n\n# âœ… æª¢æŸ¥\nprint(type(model))                 # â¬…ï¸ é€™è£¡æ‡‰è©²è¦è®Šæˆ FastLlamaForCausalLM\nprint(hasattr(model, \"fit\"))      # â¬…ï¸ æ‡‰è©²è¦ True\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"æ‰€ä»¥ä¸èƒ½ç”¨fit()","metadata":{}},{"cell_type":"markdown","source":"deberta-v3-base\n-\nTrainer + TrainingArguments","metadata":{}},{"cell_type":"code","source":"#deberta-v3-base\n!pip install -q transformers datasets accelerate  \n\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"  \n\nimport pandas as pd\nfrom datasets import Dataset\nfrom sklearn.preprocessing import LabelEncoder\n\n# è®€å–è³‡æ–™\ntrain = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/train.csv\")\n\n# æ•´ç† label\ndef get_label(row):\n    if row[\"winner_model_a\"] == 1:\n        return \"A\"\n    elif row[\"winner_model_b\"] == 1:\n        return \"B\"\n    else:\n        return \"tie\"\n\ntrain[\"label\"] = train.apply(get_label, axis=1)\n\n# çµ„åˆæˆå–®ä¸€è¼¸å…¥æ¬„ä½ï¼ˆå•å¥ + å›ç­” A + å›ç­” Bï¼‰\ndef build_input(prompt, a, b):\n    return f\"[PROMPT]: {prompt}\\n\\n[RESPONSE A]: {a}\\n\\n[RESPONSE B]: {b}\"\n\ntrain[\"text\"] = train.apply(lambda x: build_input(x[\"prompt\"], x[\"response_a\"], x[\"response_b\"]), axis=1)\n\n# å»æ‰ç¼ºæ¼\ntrain = train[[\"text\", \"label\"]].dropna().reset_index(drop=True)\n\n# Label ç·¨ç¢¼\nle = LabelEncoder()\ntrain[\"label_id\"] = le.fit_transform(train[\"label\"])\n\n# è½‰ç‚º Dataset æ ¼å¼\ndataset = Dataset.from_pandas(train).train_test_split(test_size=0.05, seed=42)  from transformers import AutoTokenizer\n\nmodel_name = \"microsoft/deberta-v3-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Tokenization function\ndef tokenize(example):\n    return tokenizer(\n        example[\"text\"],\n        padding=\"max_length\",\n        truncation=True,\n        max_length=512\n    )\n\ntokenized_dataset = dataset.map(tokenize, batched=True)\ntokenized_dataset = tokenized_dataset.remove_columns([\"text\", \"label\"])\ntokenized_dataset = tokenized_dataset.rename_column(\"label_id\", \"labels\")\ntokenized_dataset.set_format(\"torch\")  # æ¸…ç†å‰æ¬¡ç”¢ç‰©\n!rm -rf ./results\n!rm -rf ./logs\n\n# metrics function\nfrom sklearn.metrics import accuracy_score, f1_score\n\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = logits.argmax(axis=-1)\n    acc = accuracy_score(labels, predictions)\n    f1 = f1_score(labels, predictions, average=\"macro\")  # or 'weighted'\n    return {\n        \"accuracy\": acc,\n        \"f1\": f1,\n    }\n\n    \nfrom transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n\nsmall_train_dataset = tokenized_dataset[\"train\"].select(range(3000))\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    save_total_limit=1,\n    per_device_train_batch_size=8,      # âœ… å¤§ batchï¼ˆéœ€é…åˆé¡¯å¡ï¼‰\n    per_device_eval_batch_size=8,\n    num_train_epochs=8,\n    learning_rate=3e-5,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.06,\n    weight_decay=0.01,\n    logging_steps=50,\n    save_steps=2000,\n    do_train=True,\n    do_eval=True,\n    logging_dir=\"./logs\",\n    fp16=True,\n)\n\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=small_train_dataset,\n    eval_dataset=tokenized_dataset[\"test\"],\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,  # âœ… åŠ ä¸Šé€™å€‹\n\n)  # é–‹å§‹è¨“ç·´\ntrainer.train()\n\n# è¨“ç·´çµæŸå¾Œè©•ä¼°\nmetrics = trainer.evaluate()\nprint(\"ğŸ“Š è©•ä¼°æŒ‡æ¨™ï¼š\", metrics)  model.save_pretrained(\"/kaggle/working/deberta-v3-pref-model\")\ntokenizer.save_pretrained(\"/kaggle/working/deberta-v3-pref-model\")  test_text = \"[PROMPT]: ä»Šå¤©å¤©æ°£å¦‚ä½•ï¼Ÿ\\n\\n[RESPONSE A]: å¾ˆå¥½ï¼\\n\\n[RESPONSE B]: æˆ‘ä¸ç¢ºå®šã€‚\"\n\n# ç·¨ç¢¼è¼¸å…¥\ninputs = tokenizer(test_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\ninputs = {k: v.to(model.device) for k, v in inputs.items()}  # â¬…ï¸ æ¬åˆ°æ­£ç¢ºè£ç½®ï¼ˆGPUï¼‰\n\n# æ¨¡å‹é æ¸¬\noutputs = model(**inputs)\npred = outputs.logits.argmax(dim=1).item()\n\n# è§£ç¢¼ç‚º A / B / tie\nprint(\"é æ¸¬çµæœï¼š\", le.inverse_transform([pred])[0]) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  æ¨¡å‹è¨“ç·´ç´€éŒ„ï¼ˆä½¿ç”¨ 3000 ç­†è¨“ç·´è³‡æ–™ï¼‰\n\n> å‚™è¨»ï¼šå…¨éƒ¨è¨“ç·´çš†ä½¿ç”¨ç›¸åŒçš„ 3000 ç­†è¨“ç·´è³‡æ–™ã€‚è¨“ç·´æ™‚é–“è¼ƒé•·ã€‚\n\n---\n\n### Epoch 1\n- Loss ç´„åœ¨ **1.10 ~ 1.15** é–“æµ®å‹•  \n- æº–ç¢ºç‡ï¼šç´„ **38.4%**\n\n---\n\n### Epoch 3\n- åˆå§‹ Loss ç´„ç‚º **1.10**\n- å…¨ç¨‹ Loss ç¶­æŒåœ¨ **1.07 ~ 1.15** é–“ï¼Œæœ‰æ³¢å‹•ä½†ç„¡æ˜é¡¯ä¸‹é™  \n- æ¨¡å‹æœ‰åœ¨å­¸ç¿’ï¼Œä½†å°šæœªå‡ºç¾ç©©å®šä¸‹é™è¶¨å‹¢\n\n---\n\n### Epoch 5\n- å‰åŠæ®µ Loss å¤šæ•¸åœ¨ **1.10** å·¦å³  \n- è‡ª step **1200** é–‹å§‹ï¼ŒLoss å‡ºç¾æ˜é¡¯ä¸‹é™è¶¨å‹¢  \n- æœ€å¾Œ Loss è½åœ¨ **0.86 ~ 0.95** å€é–“\n\n**é©—è­‰æŒ‡æ¨™ï¼š**\n- `eval_accuracy`: **37.6%**\n- `eval_f1 (macro)`: **0.370**\n\n> Loss æœ‰æ”¶æ–‚è¶¨å‹¢ï¼Œæ¨¡å‹é–‹å§‹å­¸æœƒå€åˆ† A/B  \n> æœ‰è¼•å¾®éæ“¬åˆè·¡è±¡\n\n---\n\n### Epoch 10\n- Loss å¾åˆå§‹ **1.10** ç·©æ…¢ä¸‹é™è‡³ **1.02 ~ 0.99**\n\n**é©—è­‰æŒ‡æ¨™ï¼š**\n- `eval_loss`: **1.119**\n- `eval_accuracy`: **37.2%**\n- `eval_f1 (macro)`: **0.370**\n\n> Loss ç©©å®šä¸‹é™ï¼Œç„¡æ˜é¡¯éæ“¬åˆç¾è±¡\n","metadata":{}},{"cell_type":"markdown","source":"TinyLlama\n---","metadata":{}},{"cell_type":"code","source":"# ğŸš© 1. åˆå§‹åŒ– GPU ç’°å¢ƒ & ç™»å…¥ Hugging Face\nimport torch\ntorch.cuda.empty_cache()\ntorch.cuda.ipc_collect()\n\nimport os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\nfrom kaggle_secrets import UserSecretsClient\nfrom huggingface_hub import login\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"kaggle-llama-token\")\nlogin(token=hf_token)\n\n# ğŸš© 2. è®€å–è³‡æ–™é›†\nimport pandas as pd\nfrom datasets import Dataset\nfrom sklearn.preprocessing import LabelEncoder\n\ntrain = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/train.csv\")\n\ndef get_label(row):\n    if row[\"winner_model_a\"] == 1:\n        return \"A\"\n    elif row[\"winner_model_b\"] == 1:\n        return \"B\"\n    else:\n        return \"tie\"\n\ntrain[\"label\"] = train.apply(get_label, axis=1)\n\ndef build_input(prompt, a, b):\n    return f\"[PROMPT]: {prompt}\\n\\n[RESPONSE A]: {a}\\n\\n[RESPONSE B]: {b}\"\n\ntrain[\"text\"] = train.apply(lambda x: build_input(x[\"prompt\"], x[\"response_a\"], x[\"response_b\"]), axis=1)\ntrain = train[[\"text\", \"label\"]].dropna().reset_index(drop=True)\n\nle = LabelEncoder()\ntrain[\"label_id\"] = le.fit_transform(train[\"label\"])\n\ndataset = Dataset.from_pandas(train).train_test_split(test_size=0.05, seed=42)\n\n# ğŸš© 3. è¼‰å…¥ tokenizer å’Œ modelï¼Œä¿®æ­£ pad token å•é¡Œ\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\nmodel_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token           # âœ… è¨­å®š pad token\ntokenizer.padding_side = \"right\"                    # âœ… å°é½Šæ–¹å‘å¾ˆé‡è¦\n\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\nmodel.config.pad_token_id = tokenizer.pad_token_id  # âœ… æ˜ç¢ºæŒ‡å®š\nmodel.gradient_checkpointing_enable()  # âœ… æ¸›å°‘è¨˜æ†¶é«”ç”¨é‡\n\n# ğŸš© 4. è™•ç†è³‡æ–™\ndef preprocess_function(example):\n    return tokenizer(\n        example[\"text\"],\n        truncation=True,\n        padding='max_length',\n        max_length=128\n    )\n\ntokenized_dataset = dataset.map(preprocess_function)\ntokenized_dataset = tokenized_dataset.remove_columns([\"text\", \"label\"])\ntokenized_dataset = tokenized_dataset.rename_column(\"label_id\", \"labels\")\ntokenized_dataset.set_format(\"torch\")\n\n# ğŸš© 5. è©•ä¼° metrics\nfrom sklearn.metrics import accuracy_score, f1_score\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = logits.argmax(axis=-1)\n    acc = accuracy_score(labels, predictions)\n    f1 = f1_score(labels, predictions, average=\"macro\")\n    return {\n        \"accuracy\": acc,\n        \"f1\": f1,\n    }\n\n# ğŸš© 6. è¨­å®šè¨“ç·´åƒæ•¸\nfrom transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    save_total_limit=1,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=16,\n    num_train_epochs=3,\n    learning_rate=3e-5,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.06,\n    weight_decay=0.01,\n    logging_steps=50,\n    save_steps=2000,\n    do_train=True,\n    do_eval=True,\n    logging_dir=\"./logs\",\n    fp16=True,\n    report_to=\"none\",\n)\n\n# ğŸš© 7. è¨­å®š Trainer\nfrom transformers import Trainer, DataCollatorWithPadding\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer, pad_to_multiple_of=8)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"].select(range(1000)),  # ç‚ºäº†å¿«å–ç”¨å° subset\n    eval_dataset=tokenized_dataset[\"test\"],\n    data_collator=data_collator,\n    compute_metrics=compute_metrics\n)\n\n# ğŸš© 8. é–‹å§‹è¨“ç·´\ntrainer.train()\n\n# ğŸš© 9. è©•ä¼°çµæœ\nmetrics = trainer.evaluate()\nprint(\"ğŸ“Š è©•ä¼°çµæœï¼š\", metrics)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"ä¸€ç›´å‡ºç¾  \nValueError: Cannot handle batch sizes > 1 if no padding token is defined.\n\nLoRA + TinyLlama\n-----------","metadata":{}},{"cell_type":"code","source":"# âš™ï¸ 0. è¨­å®šç’°å¢ƒ\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\nimport torch\ntorch.cuda.empty_cache()\ntorch.cuda.ipc_collect()\n\n# âœ… HuggingFace ç™»å…¥\nfrom kaggle_secrets import UserSecretsClient\nfrom huggingface_hub import login\n\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"kaggle-llama-token\")\nlogin(token=hf_token)\n\n# âœ… å®‰è£ peftï¼ˆColab æ‰éœ€è¦ï¼‰\n# %pip install -q peft\n\n# ğŸ“¥ 1. è¼‰å…¥è³‡æ–™èˆ‡å‰è™•ç†\nimport pandas as pd\nfrom datasets import Dataset\nfrom sklearn.preprocessing import LabelEncoder\n\ndf = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/train.csv\")\n\ndef get_label(row):\n    if row[\"winner_model_a\"] == 1:\n        return \"A\"\n    elif row[\"winner_model_b\"] == 1:\n        return \"B\"\n    else:\n        return \"tie\"\n\ndf[\"label\"] = df.apply(get_label, axis=1)\ndf[\"text\"] = df.apply(lambda x: f\"[PROMPT]: {x['prompt']}\\n\\n[RESPONSE A]: {x['response_a']}\\n\\n[RESPONSE B]: {x['response_b']}\", axis=1)\ndf = df[[\"text\", \"label\"]].dropna().reset_index(drop=True)\n\nle = LabelEncoder()\ndf[\"label_id\"] = le.fit_transform(df[\"label\"])\ndataset = Dataset.from_pandas(df).train_test_split(test_size=0.05, seed=42)\n\n# ğŸ§  2. Tokenizer + Dataset\nfrom transformers import AutoTokenizer\n\nmodel_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\n\ndef preprocess(example):\n    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=256)\n\ntokenized = dataset.map(preprocess)\ntokenized = tokenized.remove_columns([\"text\", \"label\"])\ntokenized = tokenized.rename_column(\"label_id\", \"labels\")\ntokenized.set_format(\"torch\")\n\n# ğŸ”§ 3. LoRA + Model è¼‰å…¥\nfrom transformers import AutoModelForSequenceClassification\nfrom peft import LoraConfig, get_peft_model, TaskType\n\nbase_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\nbase_model.config.pad_token_id = tokenizer.pad_token_id\nbase_model.gradient_checkpointing_enable()  # âœ… æ¸›å°‘è¨˜æ†¶é«”\n\n# âœ… LoRA è¨­å®š\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    lora_dropout=0.1,\n    bias=\"none\",\n    task_type=TaskType.SEQ_CLS\n)\n\nmodel = get_peft_model(base_model, lora_config)\nmodel.print_trainable_parameters()\n\n# ğŸ§ª 4. metrics & collator\nfrom sklearn.metrics import accuracy_score, f1_score\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    preds = logits.argmax(axis=-1)\n    return {\n        \"accuracy\": accuracy_score(labels, preds),\n        \"f1\": f1_score(labels, preds, average=\"macro\"),\n    }\n\nfrom transformers import DataCollatorWithPadding\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer, pad_to_multiple_of=8)\n\n# ğŸ‹ï¸ 5. è¨“ç·´è¨­å®š\nfrom transformers import TrainingArguments, Trainer\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    save_total_limit=1,\n    per_device_train_batch_size=1,       # âœ… LoRA å¯ç”¨å° batch\n    per_device_eval_batch_size=4,\n    gradient_accumulation_steps=4,\n    num_train_epochs=2,\n    learning_rate=3e-5,\n    warmup_ratio=0.06,\n    weight_decay=0.01,\n    logging_steps=50,\n    save_steps=500,\n    fp16=True,\n    report_to=\"none\",\n)\n\n# âœ… æ¸›å°‘è¨“ç·´è³‡æ–™æ•¸é‡æ¸¬è©¦ï¼ˆä½ å¯å–æ¶ˆé€™è¡Œç”¨æ•´ä»½è³‡æ–™ï¼‰\nsmall_train = tokenized[\"train\"].select(range(500))\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=small_train,\n    eval_dataset=tokenized[\"test\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics\n)\n\ntrainer.train()\n\nmetrics = trainer.evaluate()\nprint(\"ğŸ“Š è©•ä¼°æŒ‡æ¨™ï¼š\", metrics)\n\npred = trainer.predict(tokenized[\"test\"].select([0]))\nprint(pred.predictions.argmax(axis=-1))  # çœ‹æ¨¡å‹é æ¸¬ labelï¼ˆ0, 1, 2ï¼‰\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T08:50:17.895314Z","iopub.execute_input":"2025-07-28T08:50:17.895562Z","execution_failed":"2025-07-28T13:52:05.308Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/54603 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6cbb7d9153d94fa8bcf2867d8c5ce654"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2874 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de33bcd646884ac1b2fa053e7c923833"}},"metadata":{}},{"name":"stderr","text":"2025-07-28 08:52:22.287918: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753692742.306855    1252 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753692742.312890    1252 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nSome weights of LlamaForSequenceClassification were not initialized from the model checkpoint at TinyLlama/TinyLlama-1.1B-Chat-v1.0 and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 1,132,544 || all params: 1,035,651,072 || trainable%: 0.1094\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_1252/4159978303.py:116: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nNo label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='33' max='126' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 33/126 39:21 < 1:58:03, 0.01 it/s, Epoch 0.51/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":null},{"cell_type":"markdown","source":"100ç­†è³‡æ–™è¨“ç·´ï¼š\n-------------------------\n* TrainOutput(global_step=26, training_loss=1.682946278498723, metrics={'train_runtime': 1894.004, 'train_samples_per_second': 0.106, 'train_steps_per_second': 0.014, 'total_flos': 298019350118400.0, 'train_loss': 1.682946278498723, 'epoch': 2.0}) \n\n* è©•ä¼°æŒ‡æ¨™ï¼š {'eval_loss': 1.6719685792922974, 'eval_accuracy': 0.3298538622129436, 'eval_f1': 0.32163270505624236, 'eval_runtime': 383.1717, 'eval_samples_per_second': 7.501, 'eval_steps_per_second': 0.94, 'epoch': 2.0} \n\n* [1]\n\n\næ™‚é–“ï¼šç´„30åˆ†é˜\n\n500ç­†è³‡æ–™è¨“ç·´ï¼š\n-------------------------\næ™‚é–“ï¼šç´„2.5å°æ™‚  \nå¤±æ•—åŸå› ï¼šç¶²è·¯æ–·æ‰","metadata":{}}]}