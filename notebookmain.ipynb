{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"LLM Classification Finetuning 分類微調\n-\n**任務類型：分類任務（A / B / Tie）**  \n**目標：訓練一個新的模型，去模仿人類怎麼判斷偏好。**\n\n* 模型輸入：Prompt + Response A + Response B\n* 模型輸出：判斷 A / B / tie，誰的回應較好\n","metadata":{}},{"cell_type":"markdown","source":"unsloth+llama-3-8b的錯誤\n-","metadata":{}},{"cell_type":"code","source":"!pip uninstall -y torch torchvision torchaudio\n!pip install torch==2.6.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n!pip install -U unsloth unsloth_zoo bitsandbytes\n!pip install --upgrade \"transformers>=4.53.0\"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ❗不要在這段前面 import transformers 或其他 model\nfrom unsloth import FastLanguageModel\n\nFastLanguageModel.PATCHING_MODE = \"autograd\"  # 🔥 一定要在 import 任何 transformers 前設定\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n    max_seq_length = 2048,\n    dtype = None,\n    load_in_4bit = True,\n)\n\nFastLanguageModel.for_training(model)\n\n# ✅ 檢查\nprint(type(model))                 # ⬅️ 這裡應該要變成 FastLlamaForCausalLM\nprint(hasattr(model, \"fit\"))      # ⬅️ 應該要 True\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"所以不能用fit()","metadata":{}},{"cell_type":"markdown","source":"deberta-v3-base\n-\nTrainer + TrainingArguments","metadata":{}},{"cell_type":"code","source":"#deberta-v3-base\n!pip install -q transformers datasets accelerate  \n\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"  \n\nimport pandas as pd\nfrom datasets import Dataset\nfrom sklearn.preprocessing import LabelEncoder\n\n# 讀取資料\ntrain = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/train.csv\")\n\n# 整理 label\ndef get_label(row):\n    if row[\"winner_model_a\"] == 1:\n        return \"A\"\n    elif row[\"winner_model_b\"] == 1:\n        return \"B\"\n    else:\n        return \"tie\"\n\ntrain[\"label\"] = train.apply(get_label, axis=1)\n\n# 組合成單一輸入欄位（問句 + 回答 A + 回答 B）\ndef build_input(prompt, a, b):\n    return f\"[PROMPT]: {prompt}\\n\\n[RESPONSE A]: {a}\\n\\n[RESPONSE B]: {b}\"\n\ntrain[\"text\"] = train.apply(lambda x: build_input(x[\"prompt\"], x[\"response_a\"], x[\"response_b\"]), axis=1)\n\n# 去掉缺漏\ntrain = train[[\"text\", \"label\"]].dropna().reset_index(drop=True)\n\n# Label 編碼\nle = LabelEncoder()\ntrain[\"label_id\"] = le.fit_transform(train[\"label\"])\n\n# 轉為 Dataset 格式\ndataset = Dataset.from_pandas(train).train_test_split(test_size=0.05, seed=42)  from transformers import AutoTokenizer\n\nmodel_name = \"microsoft/deberta-v3-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Tokenization function\ndef tokenize(example):\n    return tokenizer(\n        example[\"text\"],\n        padding=\"max_length\",\n        truncation=True,\n        max_length=512\n    )\n\ntokenized_dataset = dataset.map(tokenize, batched=True)\ntokenized_dataset = tokenized_dataset.remove_columns([\"text\", \"label\"])\ntokenized_dataset = tokenized_dataset.rename_column(\"label_id\", \"labels\")\ntokenized_dataset.set_format(\"torch\")  # 清理前次產物\n!rm -rf ./results\n!rm -rf ./logs\n\n# metrics function\nfrom sklearn.metrics import accuracy_score, f1_score\n\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = logits.argmax(axis=-1)\n    acc = accuracy_score(labels, predictions)\n    f1 = f1_score(labels, predictions, average=\"macro\")  # or 'weighted'\n    return {\n        \"accuracy\": acc,\n        \"f1\": f1,\n    }\n\n    \nfrom transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n\nsmall_train_dataset = tokenized_dataset[\"train\"].select(range(3000))\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    save_total_limit=1,\n    per_device_train_batch_size=8,      # ✅ 大 batch（需配合顯卡）\n    per_device_eval_batch_size=8,\n    num_train_epochs=8,\n    learning_rate=3e-5,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.06,\n    weight_decay=0.01,\n    logging_steps=50,\n    save_steps=2000,\n    do_train=True,\n    do_eval=True,\n    logging_dir=\"./logs\",\n    fp16=True,\n)\n\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=small_train_dataset,\n    eval_dataset=tokenized_dataset[\"test\"],\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,  # ✅ 加上這個\n\n)  # 開始訓練\ntrainer.train()\n\n# 訓練結束後評估\nmetrics = trainer.evaluate()\nprint(\"📊 評估指標：\", metrics)  model.save_pretrained(\"/kaggle/working/deberta-v3-pref-model\")\ntokenizer.save_pretrained(\"/kaggle/working/deberta-v3-pref-model\")  test_text = \"[PROMPT]: 今天天氣如何？\\n\\n[RESPONSE A]: 很好！\\n\\n[RESPONSE B]: 我不確定。\"\n\n# 編碼輸入\ninputs = tokenizer(test_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\ninputs = {k: v.to(model.device) for k, v in inputs.items()}  # ⬅️ 搬到正確裝置（GPU）\n\n# 模型預測\noutputs = model(**inputs)\npred = outputs.logits.argmax(dim=1).item()\n\n# 解碼為 A / B / tie\nprint(\"預測結果：\", le.inverse_transform([pred])[0]) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  模型訓練紀錄（使用 3000 筆訓練資料）\n\n> 備註：全部訓練皆使用相同的 3000 筆訓練資料。訓練時間較長。\n\n---\n\n### Epoch 1\n- Loss 約在 **1.10 ~ 1.15** 間浮動  \n- 準確率：約 **38.4%**\n\n---\n\n### Epoch 3\n- 初始 Loss 約為 **1.10**\n- 全程 Loss 維持在 **1.07 ~ 1.15** 間，有波動但無明顯下降  \n- 模型有在學習，但尚未出現穩定下降趨勢\n\n---\n\n### Epoch 5\n- 前半段 Loss 多數在 **1.10** 左右  \n- 自 step **1200** 開始，Loss 出現明顯下降趨勢  \n- 最後 Loss 落在 **0.86 ~ 0.95** 區間\n\n**驗證指標：**\n- `eval_accuracy`: **37.6%**\n- `eval_f1 (macro)`: **0.370**\n\n> Loss 有收斂趨勢，模型開始學會區分 A/B  \n> 有輕微過擬合跡象\n\n---\n\n### Epoch 10\n- Loss 從初始 **1.10** 緩慢下降至 **1.02 ~ 0.99**\n\n**驗證指標：**\n- `eval_loss`: **1.119**\n- `eval_accuracy`: **37.2%**\n- `eval_f1 (macro)`: **0.370**\n\n> Loss 穩定下降，無明顯過擬合現象\n","metadata":{}},{"cell_type":"markdown","source":"TinyLlama\n---","metadata":{}},{"cell_type":"code","source":"# 🚩 1. 初始化 GPU 環境 & 登入 Hugging Face\nimport torch\ntorch.cuda.empty_cache()\ntorch.cuda.ipc_collect()\n\nimport os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\nfrom kaggle_secrets import UserSecretsClient\nfrom huggingface_hub import login\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"kaggle-llama-token\")\nlogin(token=hf_token)\n\n# 🚩 2. 讀取資料集\nimport pandas as pd\nfrom datasets import Dataset\nfrom sklearn.preprocessing import LabelEncoder\n\ntrain = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/train.csv\")\n\ndef get_label(row):\n    if row[\"winner_model_a\"] == 1:\n        return \"A\"\n    elif row[\"winner_model_b\"] == 1:\n        return \"B\"\n    else:\n        return \"tie\"\n\ntrain[\"label\"] = train.apply(get_label, axis=1)\n\ndef build_input(prompt, a, b):\n    return f\"[PROMPT]: {prompt}\\n\\n[RESPONSE A]: {a}\\n\\n[RESPONSE B]: {b}\"\n\ntrain[\"text\"] = train.apply(lambda x: build_input(x[\"prompt\"], x[\"response_a\"], x[\"response_b\"]), axis=1)\ntrain = train[[\"text\", \"label\"]].dropna().reset_index(drop=True)\n\nle = LabelEncoder()\ntrain[\"label_id\"] = le.fit_transform(train[\"label\"])\n\ndataset = Dataset.from_pandas(train).train_test_split(test_size=0.05, seed=42)\n\n# 🚩 3. 載入 tokenizer 和 model，修正 pad token 問題\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\nmodel_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token           # ✅ 設定 pad token\ntokenizer.padding_side = \"right\"                    # ✅ 對齊方向很重要\n\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\nmodel.config.pad_token_id = tokenizer.pad_token_id  # ✅ 明確指定\nmodel.gradient_checkpointing_enable()  # ✅ 減少記憶體用量\n\n# 🚩 4. 處理資料\ndef preprocess_function(example):\n    return tokenizer(\n        example[\"text\"],\n        truncation=True,\n        padding='max_length',\n        max_length=128\n    )\n\ntokenized_dataset = dataset.map(preprocess_function)\ntokenized_dataset = tokenized_dataset.remove_columns([\"text\", \"label\"])\ntokenized_dataset = tokenized_dataset.rename_column(\"label_id\", \"labels\")\ntokenized_dataset.set_format(\"torch\")\n\n# 🚩 5. 評估 metrics\nfrom sklearn.metrics import accuracy_score, f1_score\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = logits.argmax(axis=-1)\n    acc = accuracy_score(labels, predictions)\n    f1 = f1_score(labels, predictions, average=\"macro\")\n    return {\n        \"accuracy\": acc,\n        \"f1\": f1,\n    }\n\n# 🚩 6. 設定訓練參數\nfrom transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    save_total_limit=1,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=16,\n    num_train_epochs=3,\n    learning_rate=3e-5,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.06,\n    weight_decay=0.01,\n    logging_steps=50,\n    save_steps=2000,\n    do_train=True,\n    do_eval=True,\n    logging_dir=\"./logs\",\n    fp16=True,\n    report_to=\"none\",\n)\n\n# 🚩 7. 設定 Trainer\nfrom transformers import Trainer, DataCollatorWithPadding\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer, pad_to_multiple_of=8)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"].select(range(1000)),  # 為了快取用小 subset\n    eval_dataset=tokenized_dataset[\"test\"],\n    data_collator=data_collator,\n    compute_metrics=compute_metrics\n)\n\n# 🚩 8. 開始訓練\ntrainer.train()\n\n# 🚩 9. 評估結果\nmetrics = trainer.evaluate()\nprint(\"📊 評估結果：\", metrics)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"一直出現  \nValueError: Cannot handle batch sizes > 1 if no padding token is defined.\n\nLoRA + TinyLlama\n-----------","metadata":{}},{"cell_type":"code","source":"# ⚙️ 0. 設定環境\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\nimport torch\ntorch.cuda.empty_cache()\ntorch.cuda.ipc_collect()\n\n# ✅ HuggingFace 登入\nfrom kaggle_secrets import UserSecretsClient\nfrom huggingface_hub import login\n\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"kaggle-llama-token\")\nlogin(token=hf_token)\n\n# ✅ 安裝 peft（Colab 才需要）\n# %pip install -q peft\n\n# 📥 1. 載入資料與前處理\nimport pandas as pd\nfrom datasets import Dataset\nfrom sklearn.preprocessing import LabelEncoder\n\ndf = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/train.csv\")\n\ndef get_label(row):\n    if row[\"winner_model_a\"] == 1:\n        return \"A\"\n    elif row[\"winner_model_b\"] == 1:\n        return \"B\"\n    else:\n        return \"tie\"\n\ndf[\"label\"] = df.apply(get_label, axis=1)\ndf[\"text\"] = df.apply(lambda x: f\"[PROMPT]: {x['prompt']}\\n\\n[RESPONSE A]: {x['response_a']}\\n\\n[RESPONSE B]: {x['response_b']}\", axis=1)\ndf = df[[\"text\", \"label\"]].dropna().reset_index(drop=True)\n\nle = LabelEncoder()\ndf[\"label_id\"] = le.fit_transform(df[\"label\"])\ndataset = Dataset.from_pandas(df).train_test_split(test_size=0.05, seed=42)\n\n# 🧠 2. Tokenizer + Dataset\nfrom transformers import AutoTokenizer\n\nmodel_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\n\ndef preprocess(example):\n    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=256)\n\ntokenized = dataset.map(preprocess)\ntokenized = tokenized.remove_columns([\"text\", \"label\"])\ntokenized = tokenized.rename_column(\"label_id\", \"labels\")\ntokenized.set_format(\"torch\")\n\n# 🔧 3. LoRA + Model 載入\nfrom transformers import AutoModelForSequenceClassification\nfrom peft import LoraConfig, get_peft_model, TaskType\n\nbase_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\nbase_model.config.pad_token_id = tokenizer.pad_token_id\nbase_model.gradient_checkpointing_enable()  # ✅ 減少記憶體\n\n# ✅ LoRA 設定\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    lora_dropout=0.1,\n    bias=\"none\",\n    task_type=TaskType.SEQ_CLS\n)\n\nmodel = get_peft_model(base_model, lora_config)\nmodel.print_trainable_parameters()\n\n# 🧪 4. metrics & collator\nfrom sklearn.metrics import accuracy_score, f1_score\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    preds = logits.argmax(axis=-1)\n    return {\n        \"accuracy\": accuracy_score(labels, preds),\n        \"f1\": f1_score(labels, preds, average=\"macro\"),\n    }\n\nfrom transformers import DataCollatorWithPadding\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer, pad_to_multiple_of=8)\n\n# 🏋️ 5. 訓練設定\nfrom transformers import TrainingArguments, Trainer\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    save_total_limit=1,\n    per_device_train_batch_size=1,       # ✅ LoRA 可用小 batch\n    per_device_eval_batch_size=4,\n    gradient_accumulation_steps=4,\n    num_train_epochs=2,\n    learning_rate=3e-5,\n    warmup_ratio=0.06,\n    weight_decay=0.01,\n    logging_steps=50,\n    save_steps=500,\n    fp16=True,\n    report_to=\"none\",\n)\n\n# ✅ 減少訓練資料數量測試（你可取消這行用整份資料）\nsmall_train = tokenized[\"train\"].select(range(500))\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=small_train,\n    eval_dataset=tokenized[\"test\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics\n)\n\ntrainer.train()\n\nmetrics = trainer.evaluate()\nprint(\"📊 評估指標：\", metrics)\n\npred = trainer.predict(tokenized[\"test\"].select([0]))\nprint(pred.predictions.argmax(axis=-1))  # 看模型預測 label（0, 1, 2）\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T08:50:17.895314Z","iopub.execute_input":"2025-07-28T08:50:17.895562Z","execution_failed":"2025-07-28T13:52:05.308Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/54603 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6cbb7d9153d94fa8bcf2867d8c5ce654"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2874 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de33bcd646884ac1b2fa053e7c923833"}},"metadata":{}},{"name":"stderr","text":"2025-07-28 08:52:22.287918: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753692742.306855    1252 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753692742.312890    1252 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nSome weights of LlamaForSequenceClassification were not initialized from the model checkpoint at TinyLlama/TinyLlama-1.1B-Chat-v1.0 and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 1,132,544 || all params: 1,035,651,072 || trainable%: 0.1094\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_1252/4159978303.py:116: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nNo label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='33' max='126' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 33/126 39:21 < 1:58:03, 0.01 it/s, Epoch 0.51/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":null},{"cell_type":"markdown","source":"100筆資料訓練：\n-------------------------\n* TrainOutput(global_step=26, training_loss=1.682946278498723, metrics={'train_runtime': 1894.004, 'train_samples_per_second': 0.106, 'train_steps_per_second': 0.014, 'total_flos': 298019350118400.0, 'train_loss': 1.682946278498723, 'epoch': 2.0}) \n\n* 評估指標： {'eval_loss': 1.6719685792922974, 'eval_accuracy': 0.3298538622129436, 'eval_f1': 0.32163270505624236, 'eval_runtime': 383.1717, 'eval_samples_per_second': 7.501, 'eval_steps_per_second': 0.94, 'epoch': 2.0} \n\n* [1]\n\n\n時間：約30分鐘\n\n500筆資料訓練：\n-------------------------\n時間：約2.5小時  \n失敗原因：網路斷掉","metadata":{}}]}